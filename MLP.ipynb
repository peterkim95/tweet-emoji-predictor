{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sent_corpus.csv\", \"r\") as sent_file:\n",
    "    lines = sent_file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 s, sys: 1.89 s, total: 5.14 s\n",
      "Wall time: 6.33 s\n",
      "CPU times: user 4.48 s, sys: 157 ms, total: 4.64 s\n",
      "Wall time: 4.65 s\n",
      "CPU times: user 377 ms, sys: 52.4 ms, total: 429 ms\n",
      "Wall time: 475 ms\n",
      "CPU times: user 4.82 s, sys: 834 ms, total: 5.65 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%time rows = [line.split(\",\") for line in lines if line]\n",
    "%time rows = [row[:3] + [\",\".join(row[3:])] for row in rows]\n",
    "# remove document start character \n",
    "rows[0][0] = rows[0][0][1:]\n",
    "\n",
    "sentDf_cols = ['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
    "\n",
    "\n",
    "%time sentDf_a = pd.DataFrame(rows[1:],columns=sentDf_cols)\n",
    "\n",
    "#print(sentDf)\n",
    "\n",
    "sentDf_a[[\"ItemID\",\"Sentiment\"]] = sentDf_a[[\"ItemID\",\"Sentiment\"]].astype(int)\n",
    "%time sentDf_a[\"SentimentText\"] = sentDf_a[\"SentimentText\"].apply(lambda text: text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 29.5 s, total: 1min 58s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "%time w2vM = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "dim = 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 2.53 s, total: 1min 24s\n",
      "Wall time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "%time w2vM = gensim.models.Word2Vec(sentDf[\"SentimentText\"])\n",
    "dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SentimentLabels\n",
      "0                   0\n",
      "1                   7\n",
      "2                  11\n",
      "3                   0\n",
      "4                   2\n",
      "5                   1\n",
      "6                   8\n",
      "7                   0\n",
      "8                  13\n",
      "9                   9\n",
      "10                  9\n",
      "11                 16\n",
      "12                  1\n",
      "13                 11\n",
      "14                  0\n",
      "15                  0\n",
      "16                 19\n",
      "17                  0\n",
      "18                  1\n",
      "19                  7\n",
      "20                 19\n",
      "21                  1\n",
      "22                  4\n",
      "23                  6\n",
      "24                 14\n",
      "25                  2\n",
      "26                  2\n",
      "27                  2\n",
      "28                 13\n",
      "29                  5\n",
      "...               ...\n",
      "49970               6\n",
      "49971               1\n",
      "49972               2\n",
      "49973               6\n",
      "49974               0\n",
      "49975               1\n",
      "49976               1\n",
      "49977               0\n",
      "49978               0\n",
      "49979               1\n",
      "49980               4\n",
      "49981               2\n",
      "49982              12\n",
      "49983               0\n",
      "49984               7\n",
      "49985               0\n",
      "49986               9\n",
      "49987               2\n",
      "49988               0\n",
      "49989              18\n",
      "49990               2\n",
      "49991               0\n",
      "49992               4\n",
      "49993              19\n",
      "49994              12\n",
      "49995               1\n",
      "49996               0\n",
      "49997              11\n",
      "49998               0\n",
      "49999               9\n",
      "\n",
      "[50000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+\"/fullData/\"\n",
    "with open(path+\"us_train.labels\", \"r\") as labels_file:\n",
    "    lines_test = labels_file.read().split(\"\\n\")\n",
    "    \n",
    "with open(path+\"us_trial.labels\", \"r\") as labels_file:\n",
    "    lines_trial = labels_file.read().split(\"\\n\")\n",
    "    \n",
    "labels_train = pd.DataFrame(lines_test[:], columns=['SentimentLabels'])\n",
    "labels_test = pd.DataFrame(lines_trial[:], columns=['SentimentLabels'])\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 s, sys: 6.14 s, total: 9.78 s\n",
      "Wall time: 17.1 s\n",
      "CPU times: user 80.6 ms, sys: 12 ms, total: 92.6 ms\n",
      "Wall time: 90 ms\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+\"/fullData/\"\n",
    "with open(path+\"us_train.text\", \"r\") as tweet_text_file:\n",
    "    lines = tweet_text_file.read().split(\"\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "sentDf_train = pd.DataFrame(lines[:], columns=['SentimentText'])\n",
    "%time sentDf_train[\"SentimentText\"] = sentDf_train[\"SentimentText\"].apply(lambda text: text.split())\n",
    "#print(sentDf[1:])\n",
    "    \n",
    "with open(path+\"us_trial.text\", \"r\") as tweet_text_file:\n",
    "    lines = tweet_text_file.read().split(\"\\n\")\n",
    "\n",
    "    \n",
    "sentDf_test = pd.DataFrame(lines[:], columns=['SentimentText'])\n",
    "%time sentDf_test[\"SentimentText\"] = sentDf_test[\"SentimentText\"].apply(lambda text: text.split())\n",
    "#print(sentDf[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of samples to aggregate\n",
    "N_train = int(1e5)\n",
    "N_test = labels_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 1.28 s, total: 13.5 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MEAN AGGREGATION\n",
    "tvecs_train = np.array([np.array([w2vM[t] if t in w2vM\n",
    "                                else np.zeros((dim,))\n",
    "                            for t in twt]).mean(axis=0)\n",
    "                 for twt in sentDf_train['SentimentText'][:N_train]])\n",
    "\n",
    "tvecs_test = np.array([np.array([w2vM[t] if t in w2vM\n",
    "                                else np.zeros((dim,))\n",
    "                            for t in twt]).mean(axis=0)\n",
    "                 for twt in sentDf_test['SentimentText'][:N_test]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '17' '0' ..., '9' '9' '6']\n",
      "['0' '7' '11' ..., '11' '0' '9']\n",
      "[ 0  7 11 ..., 11  0  9]\n",
      "[[-0.03525391  0.06650391  0.06716309 ..., -0.05820313 -0.02873535\n",
      "   0.03405762]\n",
      " [-0.00695801  0.06316584 -0.05490945 ..., -0.06491921  0.00937722\n",
      "  -0.03728693]\n",
      " [-0.03508301 -0.05007324 -0.04908447 ..., -0.0609375  -0.00522461\n",
      "   0.06296082]\n",
      " ..., \n",
      " [ 0.02618408  0.02658081  0.0098877  ..., -0.05737305  0.02505493\n",
      "   0.06640625]\n",
      " [-0.00427246  0.09061686  0.02342394 ..., -0.0339951   0.05254449\n",
      "   0.02824571]\n",
      " [ 0.00064087  0.01641846  0.03424072 ..., -0.02520752  0.01345825\n",
      "   0.00135803]]\n"
     ]
    }
   ],
   "source": [
    "X_train = tvecs_train[:N]\n",
    "X_test = tvecs_test[:N]\n",
    "\n",
    "print(labels_train['SentimentLabels'][:N_train].values)\n",
    "print(labels_test['SentimentLabels'][:N_test].values)\n",
    "\n",
    "y_pre_train = labels_train['SentimentLabels'][:N_train].values\n",
    "y_pre_test = labels_test['SentimentLabels'][:N_test].values\n",
    "\n",
    "y_train = np.array(y_pre_train).astype(np.int)\n",
    "y_test = np.array(y_pre_test).astype(np.int)\n",
    "\n",
    "print(y_test)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate test/train split\n",
    "ratio = 0.8\n",
    "tidx = np.random.rand(N) < ratio\n",
    "pidx = ~tidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import ensemble,svm,neural_network,discriminant_analysis\n",
    "from sklearn.metrics import roc_curve,auc,precision_recall_curve\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 137 ms, total: 12.5 s\n",
      "Wall time: 13.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier()\n",
    "rf.max_depth = 7\n",
    "\n",
    "%time rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 140\n",
    "hidden_layer_config = (num_nodes,)\n",
    "\n",
    "mlp = sklearn.neural_network.MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes= hidden_layer_config, learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "\n",
    "\n",
    "#fit to training data\n",
    "#%time mlp.fit(X[tidx],y_train[tidx])\n",
    "%time mlp.fit(X_train, y_train)\n",
    "\n",
    "#51% is baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_pre_train = model.predict(X_train)\n",
    "    y_pred_train = np.array(y_pred_pre_train).astype(np.int)\n",
    "\n",
    "    y_pred_pre_test = model.predict(X_test)\n",
    "    y_pred_test = np.array(y_pred_pre_test).astype(np.int)\n",
    "\n",
    "    num_same_train = 0\n",
    "    num_same_test = 0\n",
    "\n",
    "    for i in y:\n",
    "        if(y_pred_train[i] == y_train[i]):\n",
    "            num_same_train += 1\n",
    "        if(y_pred_test[i] == y_test[i]):\n",
    "            num_same_test += 1\n",
    "\n",
    "#    print(num_same_train)    \n",
    " #   print(num_same_test)\n",
    "\n",
    "    accuracy_train = float(num_same_train) / y_train.size\n",
    "    accuracy_test = float(num_same_test) / y_test.size\n",
    "    \n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "import pickle\n",
    "print(\"nn stats... \")\n",
    "nn_acc_train, nn_acc_test = getAccuracy(mlp, X_train, X_test, y_train, y_test)\n",
    "print(\"training accuracy: \" + str(nn_acc_train))\n",
    "print(\"test accuracy: \" + str(nn_acc_test))\n",
    "if(nn_acc_test > 0.4):\n",
    "    print(\"Good model. Saving...\")\n",
    "    filename_acc = str(nn_acc_test)[2:4]\n",
    "    filename = \"model_acc\"+filename_acc+\"hl\"+str(num_nodes)+.pkl\"\n",
    "    joblib.dump(mlp, filename)\n",
    "#    pickle.dump(mlp, open(filename, 'wb'))\n",
    "    \n",
    "print(\"model saved\")\n",
    "\n",
    "#print(\"rf stats... \")\n",
    "#rf_acc_train, rf_acc_test = getAccuracy(rf, X_train, X_test, y_train, y_test)\n",
    "#print(\"training accuracy: \" + str(rf_acc_train))\n",
    "#print(\"test accuracy: \" + str(rf_acc_test))\n",
    "\n",
    "#140 hidden layer\n",
    "#training accuracy: 0.33859\n",
    "#test accuracy: 0.69872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is not binary and pos_label is not specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-559436b206c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/sklearn/metrics/ranking.pyc\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[1;32m    509\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 510\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/sklearn/metrics/ranking.pyc\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    317\u001b[0m              \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m              np.array_equal(classes, [1]))):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data is not binary and pos_label is not specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is not binary and pos_label is not specified"
     ]
    }
   ],
   "source": [
    "n_classes = 20\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y[:i], y_pred[:i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
